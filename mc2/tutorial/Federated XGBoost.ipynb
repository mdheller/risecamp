{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secure XGBoost Tutorial\n",
    "#### RISE Camp tutorial on the Secure XGBoost project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Party XGBoost on Data Subset\n",
    "First we'll train an XGBoost model on a subset of the data. This simulates the federated setting in that a party will only have a subset of the data that's available to the central trusted server for training. We'll look at the performance of a XGBoost model that's only trained on this subset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in and examine our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data_subset = pd.read_csv('/data/msd_training_data_split.csv', sep=\",\", header=None)\n",
    "training_data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_subset = training_data_subset.iloc[:, 0]\n",
    "y_train_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train_subset = training_data_subset.iloc[:, 1:]\n",
    "x_train_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_subset = pd.read_csv('/data/msd_test_data_split.csv', sep=\",\", header=None)\n",
    "y_test_subset = test_data_subset.iloc[:, 0]\n",
    "x_test_subset = test_data_subset.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(n_estimators=40)\n",
    "model.fit(x_train_subset, y_train_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions and evaluate the model with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test_subset)\n",
    "np.sqrt(mean_squared_error(preds, y_test_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Party XGBoost on Full Dataset\n",
    "Next we will look at the performance (both in terms of speed and accuracy) of training an XGBoost model on the full dataset. Training on the full dataset makes the model much more robust, and therefore helps to motivate running XGBoost in the federated setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('/data/msd_training_data.csv', sep=\",\", header=None)\n",
    "y_train = training_data.iloc[:, 0]\n",
    "x_train = training_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('/data/msd_test_data.csv', sep=\",\", header=None)\n",
    "y_test = test_data.iloc[:, 0]\n",
    "x_test = test_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = xgb.XGBRegressor(n_estimators=40)\n",
    "full_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = full_model.predict(x_test)\n",
    "np.sqrt(mean_squared_error(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiparty XGBoost with Distributed Data\n",
    "This section will demonstrate a workflow in which each party has its own data and sends a copy of its data to the master. Therefore, all the training data is sent over the network to the master, who collects it and locally trains a model on all the data. We will also measure the number of bytes sent over the network to show the large bandwidth needed for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are a worker, run this cell\n",
    "!scp /data/training_data_1.csv <master_ip>:/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are the master, load all the data that has been sent to your machine. For example, if three other parties sent you data, make 4 calls to `read_csv()`: one for your own data and three for the other parties' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_training_data = pd.read_csv('/data/msd_training_data_split.csv', sep=\",\", header=None)\n",
    "p1_training_data = pd.read_csv('/data/msd_training_data_split.csv', sep=\",\", header=None)\n",
    "master_training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all the data in preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_training_data = pd.concat([master_training_data, p1_training_data])\n",
    "aggregated_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_agg_train = aggregated_training_data.iloc[:, 0]\n",
    "x_agg_train = aggregated_training_data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiparty_model = xgb.XGBRegressor(n_estimators=40)\n",
    "multiparty_model.fit(x_agg_train, y_agg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how are we going to set up test data?\n",
    "preds = multiparty_model.predict(x_test)\n",
    "np.sqrt(mean_squared_error(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated XGBoost\n",
    "We will now discuss running XGBoost in the federated setting. Unlike the previous exercise, in the federated setting all data stays on its respective machine. This eliminates the need to transfer over the network which incurs high overhead and requires significant bandwidth. Instead, in the federated setting in each iteration each party sends a summary of the update made to its model. The master then aggregates these updates, applies the aggregated update to its model, and broadcasts the new model to all parties. The parties then train locally with the new model and send the update to the master.\n",
    "\n",
    "### Edit hosts.config \n",
    "The `hosts.config` file should contain the IPs and ports of all workers in the federation. After loading in the `hosts.config` file, modify it to contain the IPs of your new friends! Then write the new addresses back to the file by adding a magic to the top of the cell:\n",
    "\n",
    "`%%writefile hosts.config`\n",
    "\n",
    "Make sure to delete the `# %load hosts.config` line from the cell before saving it. We'll be continually using the `%load` and `%%writefile` magics in this tutorial to edit files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load hosts.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add SSH keys\n",
    "We now need to add the master's public SSH key to all worker's `authorized_keys` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you're the master to get your SSH public key\n",
    "!cat ~/.ssh/id_rsa.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you're a worker to authorize the master to SSH into your machine.\n",
    "# Replace the <master_ssh_key> with the actual key\n",
    "!echo \"<master_ssh_key>\" >> ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Training/Eval Script\n",
    "We will now modify the script that will be run for training and evaluation. Load it in by running the following cell. The contents of the script should appear in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Job\n",
    "After modifying the script, we can start our job! We use the `start_job.sh` script with the given options to do so.\n",
    "\n",
    "The following flags must be specified when running the script.\n",
    "\n",
    "`./start_job.sh`\n",
    "\n",
    "* `-m | --worker-memory` string, specified as \"<memory>g\", e.g. 3g\n",
    "    * Amount of memory on workers allocated to job\n",
    "* `-p | --num-parties` integer\n",
    "    * Number of parties in the federation\n",
    "* `-d | --dir` string\n",
    "    * Path to created subdirectory containing job script, e.g. `/home/ubuntu/mc2/federated-xgboost/risecamp`\n",
    "* `-j | --job` string\n",
    "    * Path to job script. This should be the parameter passed into the `--dir` option concatenated with the job script file name, e.g. `/home/ubuntu/mc2/federated-xgboost/risecamp/tutorial.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./start_job.sh -p 3 -m 3g -j /home/$USER/tutorial.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
