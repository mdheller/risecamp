{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiparty XGBoost with Federated Training\n",
    "We will now discuss running XGBoost in the federated setting. Unlike the previous exercise, in the federated setting all data stays on its respective machine. This eliminates the need to transfer over the network which incurs high overhead and requires significant bandwidth. Instead, in the federated setting in each iteration each party sends a summary of the update made to its model. The central server then aggregates these updates, applies the aggregated update to its model, and broadcasts the new model to all parties. The parties then train locally with the new model and sends the update to the central server.\n",
    "\n",
    "![title](img/exercise3.png)\n",
    "\n",
    "In our project, all this is abstracted away. The central server simply starts the training, and everything else is performed automatically.\n",
    "\n",
    "Import some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "from Utils import start_job, Federation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "The central aggregator will take care of the entire training process; s/he can start the training job from his/her machine. You can take a break while the aggregator is doing training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "We'll now use the model we trained in the previous step to make predictions on our test data. Load in the federated model, preprocess your test data, and evaluate the model with the test data.\n",
    "* Test data for the Higgs boson dataset is at `/data/hb/hb_test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model_path = \"ex2_model.model\"\n",
    "multiparty_model = xgb.Booster()\n",
    "multiparty_model.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"/data/hb/hb_test.csv\"\n",
    "test_data = pd.read_csv(test_data_path, sep=\",\", header=None)\n",
    "y_test = test_data.iloc[:, 0]\n",
    "x_test = test_data.iloc[:, 1:]\n",
    "test_data = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_str = multiparty_model.eval(test_data)\n",
    "\n",
    "# Some string parsing for pretty printing\n",
    "error = float(error_str.split(\"error:\", 1)[1])\n",
    "accuracy = 1 - error\n",
    "accuracy_percent = str(accuracy * 100)[:5] + \"%\"\n",
    "print(\"Your model achieved %s accuracy \" % accuracy_percent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
