{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\">\n",
    "   <ul class=\"toc-item\" id=\"toc-level0\">\n",
    "<!--       <li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview\"><span class=\"toc-item-num\">I.&nbsp;&nbsp;</span>Overview of Tutorial</a></span></li>\n",
    "      <li><span><a href=\"#Part-1\" data-toc-modified-id=\"Part-1\"><span class=\"toc-item-num\">II.&nbsp;&nbsp;</span>Part 1</a></span></li>\n",
    "      <ul class=\"toc-item\" id=\"toc-level1\">\n",
    "         <li><span><a href=\"#Jupyter-Notebooks\" data-toc-modified-id=\"Jupyter-Notebooks\"><span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>Jupyter Notebooks</a></span></li>\n",
    "         <li>\n",
    "            <span><a href=\"#API-Overview\" data-toc-modified-id=\"API-Overview-1\"><span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>API Overview</a></span>\n",
    "            <ul class=\"toc-item\">\n",
    "               <li><span><a href=\"#Context\" data-toc-modified-id=\"Context-1.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Context</a></span></li>\n",
    "               <li><span><a href=\"#Creating-a-ClipperConnection\" data-toc-modified-id=\"Create-a-ClipperConnection-1.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Creating a ClipperConnection</a></span></li>\n",
    "               <li><span><a href=\"#Starting-Clipper\" data-toc-modified-id=\"Starting-Clipper-1.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Starting Clipper</a></span></li>\n",
    "               <li>\n",
    "                  <span><a href=\"#Deploying-a-model\" data-toc-modified-id=\"Deploying-a-model-1.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Deploying a model</a></span>\n",
    "                  <ul class=\"toc-item\">\n",
    "                     <li>\n",
    "                        <span><a href=\"#Creating-a-model\" data-toc-modified-id=\"Create-the-model-1.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Creating a model</a></span>\n",
    "                     </li>\n",
    "                     <li><span><a href=\"#Deploying-to-Clipper\" data-toc-modified-id=\"Deploying-to-Clipper-1.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Deploying to Clipper</a></span></li>\n",
    "                     <li><span><a href=\"#A-Note-About-Types-[Optional]\" data-toc-modified-id=\"A-Note-About-Types-[Optional]-1.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>A Note About Types [Optional]</a></span></li>\n",
    "                  </ul>\n",
    "               </li>\n",
    "               <li><span><a href=\"#Registering-an-Application\" data-toc-modified-id=\"Registering-an-Application-1.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Registering an Application</a></span></li>\n",
    "               <li><span><a href=\"#Inspecting-Clipper\" data-toc-modified-id=\"Inspecting-Clipper-1.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Inspecting Clipper</a></span></li>\n",
    "               <li><span><a href=\"#Updating-the-Model\" data-toc-modified-id=\"Updating-the-Model-1.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Updating the Model</a></span></li>\n",
    "               <li><span><a href=\"#Adding-Model-Replicas\" data-toc-modified-id=\"Adding-Model-Replicas-1.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Adding Model Replicas</a></span></li>\n",
    "            </ul>\n",
    "         </li>\n",
    "      </ul>\n",
    "   </ul> -->\n",
    "      <ul class=\"toc-item\" id=\"toc-level00\">\n",
    "          <li><span><a href=\"#Part-2\" data-toc-modified-id=\"Part 2\"><span class=\"toc-item-num\">III.&nbsp;&nbsp;</span>Part 2</a></span></li>\n",
    "         <ul class=\"toc-item\" id=\"toc-level11\">\n",
    "            <li>\n",
    "               <span><a href=\"#Example-Application---Image-Classification\" data-toc-modified-id=\"Example-Application---Image-Classification\"><span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>Example Application - Image Classification</a></span>\n",
    "            <li><span><a href=\"#Example-Application---Custom-Docker-Containers\" data-toc-modified-id=\"Example-Application---Custom-Docker-Containers\"><span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>Example Application - Custom Docker Containers</a></span></li>\n",
    "            <li><span><a href=\"#Restarting-Clipper\" data-toc-modified-id=\"Restarting-Clipper\"><span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>Restarting Clipper</a></span></li>\n",
    "         </ul>\n",
    "      </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains some initialization code. If you ran through [Part 1](1-Clipper-API.ipynb), then you can skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipper_admin import ClipperConnection, DockerContainerManager\n",
    "import requests, json\n",
    "clipper_conn = ClipperConnection(DockerContainerManager())\n",
    "clipper_conn.stop_all()\n",
    "clipper_conn.start_clipper()\n",
    "clipper_addr = clipper_conn.get_query_addr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Application - Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this exercise, you will deploy a pre-trained [SqueezeNet](https://arxiv.org/abs/1602.07360) model that you will download from the PyTorch model zoo to classify images.\n",
    "\n",
    "You will create an application that labels images from the ImageNet dataset.\n",
    "\n",
    "Some sample images have already been downloaded for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an application\n",
    "\n",
    "For this tutorial, create an application named \"squeezenet-classifier\". Note that Clipper allows you to create the application before deploying any models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"squeezenet-classsifier\"\n",
    "default_output = \"default\"\n",
    "\n",
    "clipper_conn.register_application(\n",
    "    name=app_name,\n",
    "    input_type=\"bytes\",\n",
    "    default_output=default_output,\n",
    "    slo_micros=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you list the applications registered with Clipper, you should see the newly registered \"squeezenet-classifier\" application show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.get_all_apps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start serving\n",
    "\n",
    "As soon as you register the application, the REST endpoint is live, even though no models have been linked yet. However, in this exercsie you will wait until you've deployed and linked a model before querying the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the pretrained model from the PyTorch model zoo\n",
    "\n",
    "We will be using a pretrained SqueezeNet model from PyTorch. Download the SqueezeNet model from the PyTorch model zoo and import it into your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Details about the model</summary>\n",
    "<p>Several common machine learning libraries, including PyTorch and TensorFlow, maintain a \"model zoo\" of pre-trained models that can be downloaded and used for inference without the need to train them. Most of the models in these model zoos are state-of-the-art deep-learning models which require an enormous amount of compute power (often several thousand GPU-hours) and expertise to train well. PyTorch provides a simple API to download and import models available in their computer vision model zoo.</p></details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "model = models.squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying the PyTorch Model\n",
    "\n",
    "Unlike the Scikit-Learn model in [Section 1](#API-Overview), PyTorch models cannot just be pickled and loaded. Instead, they must be saved using PyTorch's native serialization API. Because of this, you cannot use the generic Python model deployer to deploy the model to Clipper. Instead, you will use the Clipper PyTorch deployer to deploy it. The Docker container will load and reconstruct the model from the serialized model checkpoint when the container is started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing\n",
    "Before making the predict function, you will download some labels for the dataset to test the accuracy of the pre-trained model, and specify some preprocessing logic to transform the images into the format the model expects. The code for this cell comes from the this [tutorial](http://blog.outcome.io/pytorch-quick-start-classifying-an-image/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the preproccessing on the images:\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "   transforms.Resize(256),\n",
    "   transforms.CenterCrop(224),\n",
    "   transforms.ToTensor(),\n",
    "   normalize\n",
    "])\n",
    "\n",
    "# Then we download the labels:\n",
    "labels = {int(key):value for (key, value)\n",
    "          in requests.get('https://s3.amazonaws.com/outcome-blog/imagenet/labels.json').json().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a predict function and add metrics\n",
    "Now you can define a predict function to deploy. This predict function will wrap the model you downloaded from the model zoo as well as the preprocessing logic you just specified.\n",
    "\n",
    "You will also use Clipper's metrics reporting functionality to export some custom metrics for this model. In particular, you will report the batch size for each batch sent to the model. You can view a simple metrics dashboard automatically exported by Prometheus at port 9090 (`http:://[IP_ADDRESS]:9090`). You will also get to see a detailed dashboard in Grafana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>More info on Metrics</summary>\n",
    "<p>These metrics will be aggregated along with several system metrics and reported to [Prometheus](https://prometheus.io/), a time-series database commonly used for metrics reporting (for example, Kubernetes uses Prometheus for its own system metrics).</p>\n",
    "<p>In practice, a monitoring and alerting system such as [Grafana](https://grafana.com/) is typically layered on top of Promtheus. You will work through an example of connecting Grafana to visualize Clipper's metrics later in the tutorial.</p></details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clipper_admin.metrics as metrics\n",
    "\n",
    "def predict_torch_model(model, imgs):\n",
    "    import io\n",
    "    import PIL.Image\n",
    "    import torch\n",
    "    import clipper_admin.metrics as metrics\n",
    "    \n",
    "    metrics.add_metric(\"batch_size\", 'Gauge', 'Batch size passed to PyTorch predict function.')\n",
    "    metrics.report_metric('batch_size', len(imgs))\n",
    "    \n",
    "    # We first prepare a batch from `imgs`\n",
    "    img_tensors = []\n",
    "    for img in imgs:\n",
    "        img_tensor = preprocess(PIL.Image.open(io.BytesIO(img)))\n",
    "        img_tensor.unsqueeze_(0)\n",
    "        img_tensors.append(img_tensor)\n",
    "    img_batch = torch.cat(img_tensors)\n",
    "    \n",
    "    # We perform a forward pass\n",
    "    with torch.no_grad():\n",
    "        model_output = model.eval()(img_batch)\n",
    "        \n",
    "    # Parse Result\n",
    "    img_labels = [labels[out.data.numpy().argmax()] for out in model_output]\n",
    "        \n",
    "    return img_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Once again, Clipper must download this Docker image from the internet, so this may take a minute. Thanks for your patience.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipper_admin.deployers import pytorch as pytorch_deployer\n",
    "pytorch_deployer.deploy_pytorch_model(\n",
    "    clipper_conn,\n",
    "    name=\"pytorch-model\",\n",
    "    version=1, \n",
    "    input_type=\"bytes\", \n",
    "    func=predict_torch_model,\n",
    "    pytorch_model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.link_model_to_app(app_name=\"squeezenet-classsifier\", model_name=\"pytorch-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keeping track of metrics is an important service that Clipper offers users. Clipper reports metrics to its own Prometheus server - which can then be connected to other services, such as [Grafana](http://grafana.com). To help you visualize Clipper's metrics, this tutorial will use Grafana's API to connect Clipper's Prometheus server to Grafana and display metrics in a dashboard.*\n",
    "\n",
    "The following cells will start up Grafana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_ip = requests.get('http://ip.42.pl/raw').text\n",
    "grafana_url = 'clipper-grafana:3000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will register Clipper's Prometheus Server as a Data Source, and then import a custom dashboard for you to view your metrics with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tutorial_utils import setup_grafana\n",
    "dashboard_path = setup_grafana(grafana_url, metric_addr=clipper_conn.cm.get_metric_addr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "display(Markdown(f\"\"\"\n",
    "<a href=\"{dashboard_path}\">Click here to go to your Grafana Instance</a>\n",
    "\n",
    "Please Login with username *admin* and password *admin*\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a dashboard titled `Clipper RISE Camp Tutorial Dashboard`. If not, Click `Clipper` under the Recently Viewed Dashboards tab to see the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let send some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "req_json = json.dumps({\n",
    "        \"input\":\n",
    "        base64.b64encode(open('images/cat.jpg', \"rb\").read()).decode() # bytes to unicode\n",
    "    })\n",
    "\n",
    "response = requests.post(\n",
    "     \"http://%s/%s/predict\" % (clipper_addr, 'squeezenet-classsifier'),\n",
    "     headers={\"Content-type\": \"application/json\"},\n",
    "     data=req_json)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note that squeezenet is a deep-learning model (specifically, a convolutional neural network). Even though it was developed to be relatively fast, it may take a few seconds to return a prediction since it is running on a CPU. Deploying the model to a GPU will accelerate it substantially.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>*A Note on Batching*</summary>\n",
    "<p>It's important to note that since we are sending requests in series, there aren't multiple requests in the system at any one time, and therefore the system will not have multiple queries to batch together. You will still be able to observe some batching, however, thanks to Clipper's batching exploration algorithm. Clipper will occasionally inject additional queries into the system (when it is underloaded) to explore the throughput-latency tradeoff for each model. This exploration algorithm estimates a function that predicts query latency based on the batch size, thereby enabling Clipper to choose the largest batch size (and therefore the highest throughput) that is still capable of returning within the deadline imposed by the latency SLO. Clipper estimates a different function for each model container, allowing the system to adapt to different model characteristics and varying resource loads.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will download display images from the Internet, send them to the model, and then display the image and the result from the model. \n",
    "\n",
    "Here we show it in two flavors:\n",
    "1. Send them one-by-one. \n",
    "2. Send them in a batch. This is expected to be faster because the model utilizes batch parallelism. \n",
    "\n",
    "Note that we add noise to image so prediction will not be cached.\n",
    "\n",
    "Feel free to add the URLs of images to the `images` list to have those images be classified as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Please replace the ellipsis with your own URL's to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "import time\n",
    "from pprint import pprint\n",
    "from tutorial_utils import _add_noise_to_img\n",
    "import sys\n",
    "\n",
    "# Blue is Rehan's cat's name. The last picture is an image of his cat.\n",
    "images = ['https://raw.githubusercontent.com/ucbrise/clipper-tutorials/master/images/duck.jpg',\n",
    "          'https://github.com/ucbrise/clipper-tutorials/raw/master/images/dog.jpg',\n",
    "          'https://github.com/ucbrise/clipper-tutorials/raw/master/images/cat.jpg',\n",
    "          'https://github.com/ucbrise/clipper-tutorials/raw/master/images/blue.jpg',\n",
    "          'https://media.mnn.com/assets/images/2014/12/gray-squirrel-uc-berkeley.jpg.653x0_q80_crop-smart.jpg',\n",
    "          'http://www3.pictures.zimbio.com/gi/Oski+Grambling+v+California+W8l7cBW82vyl.jpg',\n",
    "          'https://people.eecs.berkeley.edu/~jegonzal/assets/jegonzal.jpg',\n",
    "          'https://databricks.com/wp-content/uploads/2017/12/Ion-Stoica.jpg',\n",
    "         ]\n",
    "#           ...] # TODO: Add your own URL's.\n",
    "    \n",
    "total_time_ms = 0\n",
    "for i, img in enumerate(images):\n",
    "    img_bytes = _add_noise_to_img(requests.get(url=img).content)\n",
    "    req_json = json.dumps({\n",
    "            \"input\":\n",
    "            base64.b64encode(img_bytes).decode() # bytes to unicode\n",
    "        })\n",
    "\n",
    "    begin_time = time.time()\n",
    "    response = requests.post(\n",
    "         \"http://%s/%s/predict\" % (clipper_addr, 'squeezenet-classsifier'),\n",
    "         headers={\"Content-type\": \"application/json\"},\n",
    "         data=req_json)\n",
    "    end_time = time.time()\n",
    "\n",
    "    duration_ms = (end_time-begin_time)*1000\n",
    "    total_time_ms += duration_ms\n",
    "    img_bytes = int(sys.getsizeof(img_bytes)/1024)\n",
    "    display(Markdown(f\"## > Image {i+1} ({img_bytes} kb) took {'{0:.3f}'.format(duration_ms)} miliseconds\"))\n",
    "    display(Image(url=img, width=200, height=200))\n",
    "    pprint(response.json())\n",
    "\n",
    "display(Markdown(f\"## > Predicting {len(images)} images serially took {'{0:.3f}'.format(total_time_ms/1000)} seconds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "import time\n",
    "from pprint import pprint\n",
    "import sys\n",
    "\n",
    "display(Markdown(f\"## > Now we will send these {len(images)} as a batch\"))\n",
    "\n",
    "start_time = time.time()\n",
    "input_batch = [base64.b64encode(_add_noise_to_img(requests.get(url).content)).decode() for url in images]\n",
    "end_time = time.time()\n",
    "total_size = int(sum(map(sys.getsizeof, input_batch))/1024)\n",
    "duration = end_time - start_time\n",
    "display(Markdown(f\"## > {total_size}kb downloaded in {'{0:.3f}'.format(duration)} s\"))\n",
    "\n",
    "req_json = json.dumps({\n",
    "        \"input_batch\":\n",
    "             input_batch\n",
    "    })\n",
    "\n",
    "start_time = time.time()\n",
    "response = requests.post(\n",
    "     \"http://%s/%s/predict\" % (clipper_addr, 'squeezenet-classsifier'),\n",
    "     headers={\"Content-type\": \"application/json\"},\n",
    "     data=req_json)\n",
    "end_time = time.time()\n",
    "\n",
    "duration_ms = end_time-start_time\n",
    "display(Markdown(f\"## > It takes {'{0:.3f}'.format(duration_ms)} seconds to predict this batch!\"))\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Application - Custom Docker Containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final example, you will learn how to create a custom Docker model container. Custom Docker containers can be utilized when model containers rely on dependencies that cannot be installed with Pip. In this example, you will deploying a model developed in a pure C framework that must be compiled from source within the Docker container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>**Click here for more details on the Dockerfile and model you'll be using!**</summary>\n",
    "<div class=\"text_cell_render rendered_html\" tabindex=\"-1\"><p>In this final example, you will learn how to create a custom Docker model container. Custom Docker containers can be utilized when model containers rely on dependencies that cannot be installed with Pip. In this example, you will deploying a model developed in a pure C framework that must be compiled from source within the Docker container.</p>\n",
    "<p>You will deploy the <a href=\"https://pjreddie.com/darknet/yolo/\" target=\"_blank\">YoloV3 Real Time Object Detection System</a> to predict both <em>labels and bounding boxes</em> for objects within an image. This is a harder machine learning task than the image classification task that you performed with SqueezeNet.</p>\n",
    "<p>YOLO (You Only Look Once) is a state-of-the-art object detection model, but unfortunately it does not expose a native Python API. Instead, it must be compiled from source and executed as a binary executable, not as a library. You will clone the repo (in this case, a fork we created with some minor modifications to extract the bounding box coordinates instead of directly drawing the on the image) and compile it, all within the Docker image.</p>\n",
    "<p>The first step in creating a Docker image is to write a <a href=\"https://docs.docker.com/engine/reference/builder/\" target=\"_blank\">Dockerfile</a>. This is like the source code for the image. After you have written the Dockerfile, you will build it. Unlike when you compile source code, building a Docker image actually runs the program specified in the Dockerfile. The end result is a Docker image -- a binary object that contains the results of executing the instructions in the Dockerfile sequentially from top to bottom. </p>\n",
    "<p>For your convenience, we have already written a Dockerfile for you called <code>PyDarknetDockerfile</code> and included it in the tutorial. However, before you build it you will read about each step in the Dockerfile, providing a blueprint for you to define your own custom Clipper model containers in the future.</p>\n",
    "<p>Here are the full contents of the Dockerfile:</p>\n",
    "<pre><code class=\"cm-s-ipython language-dockerfile\"><span class=\"cm-variable-2\" style=\"color:green\"><strong>FROM</strong></span> clipper/python36-closure-container:0.3\n",
    "\n",
    "<span class=\"cm-comment\"># Install Git</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> apt-get update \\\n",
    "    &amp;&amp; apt-get install -y git\n",
    "\n",
    "<span class=\"cm-comment\"># Install cURL</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> apt-get install -y curl\n",
    "\n",
    "<span class=\"cm-comment\"># Clone Darknet Repo</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> git clone https://github.com/RehanSD/darknet.git /tmp/darknet\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> mv /tmp/darknet/* .\n",
    "\n",
    "<span class=\"cm-comment\"># Make Darknet Project</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> make -j16\n",
    "\n",
    "<span class=\"cm-comment\">#Download Weights</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> curl -o yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights\n",
    "</code></pre>\n",
    "<p>In the following sections, we will break down each part of the file.</p>\n",
    "</div>\n",
    "<div class=\"text_cell_render rendered_html\" tabindex=\"-1\"><p>The first line of the file ensures that you are starting off with a Docker image with all of the necessary Clipper dependencies installed. In the rest of the Dockerfile, you are extending the Clipper-provided container to simply install some additional dependencies.</p>\n",
    "<pre><code class=\"cm-s-ipython language-dockerfile\"><span class=\"cm-variable-2\" style=\"color:green\"><strong>FROM</strong></span> clipper/python36-closure-container:0.3\n",
    "</code></pre>\n",
    "<p>The next few lines install the basic dependencies we need in the container - git to clone the repo, annd curl to download the pretrained yolo model weights.</p>\n",
    "<pre><code class=\"cm-s-ipython language-dockerfile\"><span class=\"cm-comment\"># Install Git</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> apt-get update \\\n",
    "    &amp;&amp; apt-get install -y git\n",
    "\n",
    "<span class=\"cm-comment\"># Install cURL</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> apt-get install -y curl\n",
    "</code></pre>\n",
    "<p>The last few lines clone the darknet repo, compile it, and then download the pretrained model weights. We are using <code>yolov3-tiny</code> because it offers a good trade off between accuracy and inference speed on a CPU. (For a latency-sensitive production use case, you might run it on GPU.)</p>\n",
    "<pre><code class=\"cm-s-ipython language-dockerfile\"><span class=\"cm-comment\"># Clone Darknet Repo</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> git clone https://github.com/RehanSD/darknet.git /tmp/darknet\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> mv /tmp/darknet/* .\n",
    "\n",
    "<span class=\"cm-comment\"># Make Darknet Project</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> make -j16\n",
    "\n",
    "<span class=\"cm-comment\">#Download Weights</span>\n",
    "<span class=\"cm-variable-2\" style=\"color:green\"><strong>RUN</strong></span> curl -o yolov3-tiny.weights https://pjreddie.com/media/files/yolov3-tiny.weights\n",
    "</code></pre>\n",
    "<p>Now that you have defined the image, you can build it with the following shell command.</p>\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t clipper/darknet-yolov3-container -f PyDarknetDockerfile ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict Function\n",
    "Now we build the predict function. We must first deserialize the image that we serialized in our request - similar to how we did in the PyTorch example, and then write it to a file to run darknet on it. Since darknet is a C executable, we must call it using the subprocess API. It is not strictly neccessary to print out the output of calling darknet, but it is useful to have the output for the sake of debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_pred(imgs):\n",
    "    import base64\n",
    "    import io\n",
    "    import os\n",
    "    import tempfile\n",
    "    import subprocess\n",
    "\n",
    "    num_imgs = len(imgs)\n",
    "    ret_coords = []\n",
    "    predict_procs = []\n",
    "    file_names = []\n",
    "    \n",
    "    # First, we save the images to file\n",
    "    for i in range(num_imgs):\n",
    "        # Create a temp file to write to\n",
    "        tmp = tempfile.NamedTemporaryFile('wb', delete=False, suffix='.jpg')\n",
    "        tmp.write(io.BytesIO(imgs[i]).getvalue())\n",
    "        tmp.close()\n",
    "        file_names.append(tmp.name)\n",
    "    \n",
    "    # Second, we call ./darknet executable to detect objects in images.\n",
    "    #   This is done in parallel.\n",
    "    for file_name in file_names:\n",
    "        process = subprocess.Popen(\n",
    "            ['./darknet',\n",
    "             'detector',\n",
    "             'test',\n",
    "             './cfg/coco.data',\n",
    "             './cfg/yolov3-tiny.cfg',\n",
    "             './yolov3-tiny.weights',\n",
    "             file_name,\n",
    "             '-json',\n",
    "             '-dont_show',\n",
    "             '-ext_output', '>',\n",
    "             '{}.txt'.format(file_name+'_result')], stdout=subprocess.PIPE)\n",
    "        predict_procs.append(process)\n",
    "        \n",
    "    # Lastly, we wait for all process to finished and return stdout of each process\n",
    "    for process in predict_procs:\n",
    "        process.wait()\n",
    "        ret_coords += [' '.join(map(lambda byte_str: byte_str.decode(), process.stdout))]\n",
    "\n",
    "    return ret_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not be concerned if this cell takes a couple of seconds to run.\n",
    "from clipper_admin.deployers import python as python_deployer\n",
    "python_deployer.deploy_python_closure(\n",
    "    clipper_conn,\n",
    "    name=\"yolov3\",  # The name of the model in Clipper\n",
    "    version=1,  # A unique identifier to assign to this model.\n",
    "    input_type=\"bytes\",  # The type of data the model function expects as input\n",
    "    func=yolo_pred, # The model function to deploy\n",
    "    base_image='clipper/darknet-yolov3-container'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.register_application(\n",
    "    name=\"darknet-app\",\n",
    "    input_type=\"bytes\",\n",
    "    default_output=\"Default\",\n",
    "    slo_micros=10000000 # 10 seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipper_conn.link_model_to_app(app_name=\"darknet-app\", model_name=\"yolov3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note the request may take a couple of seconds to return, as we are running a largeish model\n",
    "# on a CPU, which is slow.\n",
    "import requests, json\n",
    "from pprint import pprint\n",
    "\n",
    "url = \"http://%s/darknet-app/predict\" % clipper_addr\n",
    "req_json = json.dumps({\n",
    "    \"input\":\n",
    "    base64.b64encode(open('images/dog.jpg', \"rb\").read()).decode() # bytes to unicode\n",
    "})\n",
    "headers = {'Content-type': 'application/json'}\n",
    "r = requests.post(url, headers=headers, data=req_json)\n",
    "\n",
    "# Let's see what does YoloV3-tiny return\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the result\n",
    "from tutorial_utils import plot_bbox, predict_and_plot_url, predict_and_plot\n",
    "%matplotlib inline\n",
    "plot_bbox('images/dog.jpg', r.json()['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment with other images under `images/*.jpg`, or to use the `predict_and_plot_url` function with URLs of different images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note the request may take a couple of seconds to return, as we are running a largeish model\n",
    "# on a CPU, which is slow.\n",
    "# im_name = \"images/detection/*.jpg\"\n",
    "im_name = \"images/cityscapes-3.jpg\"\n",
    "predict_and_plot(im_name, clipper_addr)\n",
    "predict_and_plot_url('https://github.com/ucbrise/clipper-tutorials/raw/master/images/detection/baseball-boy.jpg', clipper_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've sent some images, you should check out Grafana again to see some interesting metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, Markdown\n",
    "display(Markdown(f\"\"\"\n",
    "<a href=\"{dashboard_path}\">Click here to go to your Grafana Instance</a>\n",
    "\n",
    "Please Login with username *admin* and password *admin*\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Clipper\n",
    "If you run into issues and want to completely stop Clipper, you can do this by calling [`ClipperConnection.stop_all()`](http://docs.clipper.ai/en/latest/#clipper_admin.ClipperConnection.stop_all).\n",
    "\n",
    "Uncomment the following cell to complete stop Clipper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipper_conn.stop_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you list all the Docker containers a final time, you should see that all of the Clipper containers have been stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps --filter label=ai.clipper.container.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now call `clipper_conn.start_clipper()` again without running into errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for participating in this tutorial! Please fill the [feedback form](https://goo.gl/forms/gwPcOWRdnbIaWv1L2) to tell us your interests in Clipper. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
